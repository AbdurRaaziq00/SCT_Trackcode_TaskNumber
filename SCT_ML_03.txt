# Import necessary libraries
import numpy as np
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# --- 1. Generate a Synthetic Dataset ---
# In a real-world scenario, you would load your images from the Kaggle dataset.
# For this example, we'll simulate the data.

# Let's assume our "images" are simple 2D arrays (e.g., 10x10 pixels).
# We'll represent each image as a flattened 100-element vector.
# Class 0 will be "dogs," and Class 1 will be "cats."

# Create a function to generate a synthetic image
def create_synthetic_image(is_cat):
    """
    Generates a synthetic 10x10 image.
    Cats will have a higher average pixel value.
    Dogs will have a lower average pixel value.
    """
    if is_cat:
        # Cats: Brighter images
        return np.random.rand(10, 10) * 0.8 + 0.2
    else:
        # Dogs: Darker images
        return np.random.rand(10, 10) * 0.5

# Generate 100 synthetic dog images (class 0) and 100 cat images (class 1)
num_samples = 200
X = [] # List to hold our image data
y = [] # List to hold our labels (0 for dog, 1 for cat)

for i in range(num_samples // 2):
    X.append(create_synthetic_image(is_cat=False).flatten()) # Dog
    y.append(0)
    X.append(create_synthetic_image(is_cat=True).flatten())  # Cat
    y.append(1)

# Convert lists to numpy arrays
X = np.array(X)
y = np.array(y)

print(f"Dataset created with {X.shape[0]} samples.")
print(f"Each sample has {X.shape[1]} features (pixels).")

# --- 2. Pre-processing and Data Splitting ---
# We'll split the data into training and testing sets.
# This is a critical step to ensure our model generalizes well to unseen data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# --- 3. Model Training ---
# Initialize the Support Vector Machine classifier
# We'll use a Radial Basis Function (RBF) kernel, which is a good general-purpose choice.
svm_classifier = svm.SVC(kernel='rbf', gamma='scale')

# Train the model on the training data
print("\nTraining the SVM classifier...")
svm_classifier.fit(X_train, y_train)
print("Training complete.")

# --- 4. Evaluation ---
# Make predictions on the test set
y_pred = svm_classifier.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

print(f"\nModel Accuracy: {accuracy * 100:.2f}%")

# --- Optional: Visualize a few predictions ---
# Let's show a few images and their predicted labels
num_to_show = 5
fig, axes = plt.subplots(1, num_to_show, figsize=(15, 3))

for i in range(num_to_show):
    # Select a random image from the test set
    idx = np.random.randint(0, len(X_test))
    image = X_test[idx].reshape(10, 10)
    label = "Cat" if y_test[idx] == 1 else "Dog"
    prediction = "Cat" if y_pred[idx] == 1 else "Dog"

    axes[i].imshow(image, cmap='gray')
    axes[i].set_title(f"True: {label}\nPred: {prediction}")
    axes[i].axis('off')

plt.tight_layout()
plt.show()